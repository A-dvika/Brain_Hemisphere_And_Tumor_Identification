{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f043532",
   "metadata": {},
   "source": [
    "## Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f65f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "from ultralytics.data import YOLODataset\n",
    "from ultralytics.data.augment import Compose, Format, v8_transforms\n",
    "from ultralytics.models.yolo.detect import DetectionValidator\n",
    "from ultralytics.utils import colorstr, ops\n",
    "\n",
    "__all__ = 'RTDETRValidator',  # tuple or list\n",
    "\n",
    "\n",
    "class RTDETRDataset(YOLODataset):\n",
    "    \"\"\"\n",
    "    Real-Time DEtection and TRacking (RT-DETR) dataset class extending the base YOLODataset class.\n",
    "\n",
    "    This specialized dataset class is designed for use with the RT-DETR object detection model and is optimized for\n",
    "    real-time detection and tracking tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, data=None, **kwargs):\n",
    "        \"\"\"Initialize the RTDETRDataset class by inheriting from the YOLODataset class.\"\"\"\n",
    "        super().__init__(*args, data=data, use_segments=False, use_keypoints=False, **kwargs)\n",
    "\n",
    "    # NOTE: add stretch version load_image for RTDETR mosaic\n",
    "    def load_image(self, i, rect_mode=False):\n",
    "        \"\"\"Loads 1 image from dataset index 'i', returns (im, resized hw).\"\"\"\n",
    "        return super().load_image(i=i, rect_mode=rect_mode)\n",
    "\n",
    "    def build_transforms(self, hyp=None):\n",
    "        \"\"\"Temporary, only for evaluation.\"\"\"\n",
    "        if self.augment:\n",
    "            hyp.mosaic = hyp.mosaic if self.augment and not self.rect else 0.0\n",
    "            hyp.mixup = hyp.mixup if self.augment and not self.rect else 0.0\n",
    "            transforms = v8_transforms(self, self.imgsz, hyp, stretch=True)\n",
    "        else:\n",
    "            # transforms = Compose([LetterBox(new_shape=(self.imgsz, self.imgsz), auto=False, scaleFill=True)])\n",
    "            transforms = Compose([])\n",
    "        transforms.append(\n",
    "            Format(bbox_format='xywh',\n",
    "                   normalize=True,\n",
    "                   return_mask=self.use_segments,\n",
    "                   return_keypoint=self.use_keypoints,\n",
    "                   batch_idx=True,\n",
    "                   mask_ratio=hyp.mask_ratio,\n",
    "                   mask_overlap=hyp.overlap_mask))\n",
    "        return transforms\n",
    "\n",
    "\n",
    "class RTDETRValidator(DetectionValidator):\n",
    "    \"\"\"\n",
    "    RTDETRValidator extends the DetectionValidator class to provide validation capabilities specifically tailored for\n",
    "    the RT-DETR (Real-Time DETR) object detection model.\n",
    "\n",
    "    The class allows building of an RTDETR-specific dataset for validation, applies Non-maximum suppression for\n",
    "    post-processing, and updates evaluation metrics accordingly.\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        from ultralytics.models.rtdetr import RTDETRValidator\n",
    "\n",
    "        args = dict(model='rtdetr-l.pt', data='coco8.yaml')\n",
    "        validator = RTDETRValidator(args=args)\n",
    "        validator()\n",
    "        ```\n",
    "\n",
    "    Note:\n",
    "        For further details on the attributes and methods, refer to the parent DetectionValidator class.\n",
    "    \"\"\"\n",
    "\n",
    "    def build_dataset(self, img_path, mode='val', batch=None):\n",
    "        \"\"\"\n",
    "        Build an RTDETR Dataset.\n",
    "\n",
    "        Args:\n",
    "            img_path (str): Path to the folder containing images.\n",
    "            mode (str): `train` mode or `val` mode, users are able to customize different augmentations for each mode.\n",
    "            batch (int, optional): Size of batches, this is for `rect`. Defaults to None.\n",
    "        \"\"\"\n",
    "        return RTDETRDataset(\n",
    "            img_path=img_path,\n",
    "            imgsz=self.args.imgsz,\n",
    "            batch_size=batch,\n",
    "            augment=False,  # no augmentation\n",
    "            hyp=self.args,\n",
    "            rect=False,  # no rect\n",
    "            cache=self.args.cache or None,\n",
    "            prefix=colorstr(f'{mode}: '),\n",
    "            data=self.data)\n",
    "\n",
    "    def postprocess(self, preds):\n",
    "        \"\"\"Apply Non-maximum suppression to prediction outputs.\"\"\"\n",
    "        bs, _, nd = preds[0].shape\n",
    "        bboxes, scores = preds[0].split((4, nd - 4), dim=-1)\n",
    "        bboxes *= self.args.imgsz\n",
    "        outputs = [torch.zeros((0, 6), device=bboxes.device)] * bs\n",
    "        for i, bbox in enumerate(bboxes):  # (300, 4)\n",
    "            bbox = ops.xywh2xyxy(bbox)\n",
    "            score, cls = scores[i].max(-1)  # (300, )\n",
    "            # Do not need threshold for evaluation as only got 300 boxes here\n",
    "            # idx = score > self.args.conf\n",
    "            pred = torch.cat([bbox, score[..., None], cls[..., None]], dim=-1)  # filter\n",
    "            # Sort by confidence to correctly get internal metrics\n",
    "            pred = pred[score.argsort(descending=True)]\n",
    "            outputs[i] = pred  # [idx]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def update_metrics(self, preds, batch):\n",
    "        \"\"\"Metrics.\"\"\"\n",
    "        for si, pred in enumerate(preds):\n",
    "            idx = batch['batch_idx'] == si\n",
    "            cls = batch['cls'][idx]\n",
    "            bbox = batch['bboxes'][idx]\n",
    "            nl, npr = cls.shape[0], pred.shape[0]  # number of labels, predictions\n",
    "            shape = batch['ori_shape'][si]\n",
    "            correct_bboxes = torch.zeros(npr, self.niou, dtype=torch.bool, device=self.device)  # init\n",
    "            self.seen += 1\n",
    "\n",
    "            if npr == 0:\n",
    "                if nl:\n",
    "                    self.stats.append((correct_bboxes, *torch.zeros((2, 0), device=self.device), cls.squeeze(-1)))\n",
    "                    if self.args.plots:\n",
    "                        self.confusion_matrix.process_batch(detections=None, labels=cls.squeeze(-1))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            if self.args.single_cls:\n",
    "                pred[:, 5] = 0\n",
    "            predn = pred.clone()\n",
    "            predn[..., [0, 2]] *= shape[1] / self.args.imgsz  # native-space pred\n",
    "            predn[..., [1, 3]] *= shape[0] / self.args.imgsz  # native-space pred\n",
    "\n",
    "            # Evaluate\n",
    "            if nl:\n",
    "                tbox = ops.xywh2xyxy(bbox)  # target boxes\n",
    "                tbox[..., [0, 2]] *= shape[1]  # native-space pred\n",
    "                tbox[..., [1, 3]] *= shape[0]  # native-space pred\n",
    "                labelsn = torch.cat((cls, tbox), 1)  # native-space labels\n",
    "                # NOTE: To get correct metrics, the inputs of `_process_batch` should always be float32 type.\n",
    "                correct_bboxes = self._process_batch(predn.float(), labelsn)\n",
    "                # TODO: maybe remove these `self.` arguments as they already are member variable\n",
    "                if self.args.plots:\n",
    "                    self.confusion_matrix.process_batch(predn, labelsn)\n",
    "            self.stats.append((correct_bboxes, pred[:, 4], pred[:, 5], cls.squeeze(-1)))  # (conf, pcls, tcls)\n",
    "\n",
    "            # Save\n",
    "            if self.args.save_json:\n",
    "                self.pred_to_json(predn, batch['im_file'][si])\n",
    "            if self.args.save_txt:\n",
    "                file = self.save_dir / 'labels' / f'{Path(batch[\"im_file\"][si]).stem}.txt'\n",
    "                self.save_one_txt(predn, self.args.save_conf, shape, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129c2ff",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ac3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from copy import copy\n",
    "\n",
    "import torch\n",
    "\n",
    "from ultralytics.models.yolo.detect import DetectionTrainer\n",
    "from ultralytics.nn.tasks import RTDETRDetectionModel\n",
    "from ultralytics.utils import RANK, colorstr\n",
    "\n",
    "# from val import RTDETRDataset, RTDETRValidator\n",
    "\n",
    "\n",
    "class RTDETRTrainer(DetectionTrainer):\n",
    "    \"\"\"\n",
    "    Trainer class for the RT-DETR model developed by Baidu for real-time object detection. Extends the DetectionTrainer\n",
    "    class for YOLO to adapt to the specific features and architecture of RT-DETR. This model leverages Vision\n",
    "    Transformers and has capabilities like IoU-aware query selection and adaptable inference speed.\n",
    "\n",
    "    Notes:\n",
    "        - F.grid_sample used in RT-DETR does not support the `deterministic=True` argument.\n",
    "        - AMP training can lead to NaN outputs and may produce errors during bipartite graph matching.\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        from ultralytics.models.rtdetr.train import RTDETRTrainer\n",
    "\n",
    "        args = dict(model='rtdetr-l.yaml', data='coco8.yaml', imgsz=640, epochs=3)\n",
    "        trainer = RTDETRTrainer(overrides=args)\n",
    "        trainer.train()\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def get_model(self, cfg=None, weights=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Initialize and return an RT-DETR model for object detection tasks.\n",
    "\n",
    "        Args:\n",
    "            cfg (dict, optional): Model configuration. Defaults to None.\n",
    "            weights (str, optional): Path to pre-trained model weights. Defaults to None.\n",
    "            verbose (bool): Verbose logging if True. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            (RTDETRDetectionModel): Initialized model.\n",
    "        \"\"\"\n",
    "        model = RTDETRDetectionModel(cfg, nc=self.data['nc'], verbose=verbose and RANK == -1)\n",
    "        if weights:\n",
    "            model.load(weights)\n",
    "        return model\n",
    "\n",
    "    def build_dataset(self, img_path, mode='val', batch=None):\n",
    "        \"\"\"\n",
    "        Build and return an RT-DETR dataset for training or validation.\n",
    "\n",
    "        Args:\n",
    "            img_path (str): Path to the folder containing images.\n",
    "            mode (str): Dataset mode, either 'train' or 'val'.\n",
    "            batch (int, optional): Batch size for rectangle training. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            (RTDETRDataset): Dataset object for the specific mode.\n",
    "        \"\"\"\n",
    "        return RTDETRDataset(img_path=img_path,\n",
    "                             imgsz=self.args.imgsz,\n",
    "                             batch_size=batch,\n",
    "                             augment=mode == 'train',\n",
    "                             hyp=self.args,\n",
    "                             rect=False,\n",
    "                             cache=self.args.cache or None,\n",
    "                             prefix=colorstr(f'{mode}: '),\n",
    "                             data=self.data)\n",
    "\n",
    "    def get_validator(self):\n",
    "        \"\"\"\n",
    "        Returns a DetectionValidator suitable for RT-DETR model validation.\n",
    "\n",
    "        Returns:\n",
    "            (RTDETRValidator): Validator object for model validation.\n",
    "        \"\"\"\n",
    "        self.loss_names = 'giou_loss', 'cls_loss', 'l1_loss'\n",
    "        return RTDETRValidator(self.test_loader, save_dir=self.save_dir, args=copy(self.args))\n",
    "\n",
    "    def preprocess_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Preprocess a batch of images. Scales and converts the images to float format.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): Dictionary containing a batch of images, bboxes, and labels.\n",
    "\n",
    "        Returns:\n",
    "            (dict): Preprocessed batch.\n",
    "        \"\"\"\n",
    "        batch = super().preprocess_batch(batch)\n",
    "        bs = len(batch['img'])\n",
    "        batch_idx = batch['batch_idx']\n",
    "        gt_bbox, gt_class = [], []\n",
    "        for i in range(bs):\n",
    "            gt_bbox.append(batch['bboxes'][batch_idx == i].to(batch_idx.device))\n",
    "            gt_class.append(batch['cls'][batch_idx == i].to(device=batch_idx.device, dtype=torch.long))\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ff27a",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc243ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
    "\n",
    "import torch\n",
    "\n",
    "from ultralytics.data.augment import LetterBox\n",
    "from ultralytics.engine.predictor import BasePredictor\n",
    "from ultralytics.engine.results import Results\n",
    "from ultralytics.utils import ops\n",
    "\n",
    "\n",
    "class RTDETRPredictor(BasePredictor):\n",
    "    \"\"\"\n",
    "    A class extending the BasePredictor class for prediction based on an RT-DETR detection model.\n",
    "\n",
    "    Example:\n",
    "        ```python\n",
    "        from ultralytics.utils import ASSETS\n",
    "        from ultralytics.models.rtdetr import RTDETRPredictor\n",
    "\n",
    "        args = dict(model='rtdetr-l.pt', source=ASSETS)\n",
    "        predictor = RTDETRPredictor(overrides=args)\n",
    "        predictor.predict_cli()\n",
    "        ```\n",
    "    \"\"\"\n",
    "\n",
    "    def postprocess(self, preds, img, orig_imgs):\n",
    "        \"\"\"Postprocess predictions and returns a list of Results objects.\"\"\"\n",
    "        nd = preds[0].shape[-1]\n",
    "        bboxes, scores = preds[0].split((4, nd - 4), dim=-1)\n",
    "\n",
    "        if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list\n",
    "            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)\n",
    "\n",
    "        results = []\n",
    "        for i, bbox in enumerate(bboxes):  # (300, 4)\n",
    "            bbox = ops.xywh2xyxy(bbox)\n",
    "            score, cls = scores[i].max(-1, keepdim=True)  # (300, 1)\n",
    "            idx = score.squeeze(-1) > self.args.conf  # (300, )\n",
    "            if self.args.classes is not None:\n",
    "                idx = (cls == torch.tensor(self.args.classes, device=cls.device)).any(1) & idx\n",
    "            pred = torch.cat([bbox, score, cls], dim=-1)[idx]  # filter\n",
    "            orig_img = orig_imgs[i]\n",
    "            oh, ow = orig_img.shape[:2]\n",
    "            pred[..., [0, 2]] *= ow\n",
    "            pred[..., [1, 3]] *= oh\n",
    "            img_path = self.batch[0][i]\n",
    "            results.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))\n",
    "        return results\n",
    "\n",
    "    def pre_transform(self, im):\n",
    "        \"\"\"Pre-transform input image before inference.\n",
    "\n",
    "        Args:\n",
    "            im (List(np.ndarray)): (N, 3, h, w) for tensor, [(h, w, 3) x N] for list.\n",
    "\n",
    "        Notes: The size must be square(640) and scaleFilled.\n",
    "\n",
    "        Returns:\n",
    "            (list): A list of transformed imgs.\n",
    "        \"\"\"\n",
    "        letterbox = LetterBox(self.imgsz, auto=False, scaleFill=True)\n",
    "        return [letterbox(image=x) for x in im]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794e1900",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52fb4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics YOLO ðŸš€, AGPL-3.0 license\n",
    "\"\"\"\n",
    "RT-DETR model interface\n",
    "\"\"\"\n",
    "from ultralytics.engine.model import Model\n",
    "from ultralytics.nn.tasks import RTDETRDetectionModel\n",
    "\n",
    "# from .predict import RTDETRPredictor\n",
    "# from .train import RTDETRTrainer\n",
    "# from .val import RTDETRValidator\n",
    "\n",
    "\n",
    "class RTDETR(Model):\n",
    "    \"\"\"\n",
    "    RTDETR model interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model='rtdetr-l.pt') -> None:\n",
    "        if model and model.split('.')[-1] not in ('pt', 'yaml', 'yml'):\n",
    "            raise NotImplementedError('RT-DETR only supports creating from *.pt file or *.yaml file.')\n",
    "        super().__init__(model=model, task='detect')\n",
    "\n",
    "    @property\n",
    "    def task_map(self):\n",
    "        return {\n",
    "            'detect': {\n",
    "                'predictor': RTDETRPredictor,\n",
    "                'validator': RTDETRValidator,\n",
    "                'trainer': RTDETRTrainer,\n",
    "                'model': RTDETRDetectionModel}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5755593",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decde89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.182  Python-3.11.3 torch-2.1.2 CUDA:0 (NVIDIA GeForce RTX 3050 Laptop GPU, 4096MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=rtdetr-l.yaml, data=C://Users//HP//Desktop//Ultralytics_RTDeTr//coco8.yaml, epochs=1, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train13\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs\\detect\\train13', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=16\n",
      "WARNING  no model scale passed. Assuming scale='l'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
      "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
      "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
      "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
      "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
      "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
      "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
      "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
      " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n",
      " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
      " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
      " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
      " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 28        [21, 24, 27]  1   7334732  ultralytics.nn.modules.head.RTDETRDecoder    [16, [256, 256, 256]]         \n",
      "rtdetr-l summary: 673 layers, 32838956 parameters, 32838956 gradients\n",
      "\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\NEVER DELETE\\Advika_Ab_Project\\Detection\\Train\\labels.cache... 72390 images, 37871 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 72390/72390 \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "args = dict(model='rtdetr-l.yaml', data=\"C://Users//HP//Desktop//Ultralytics_RTDeTr//coco8.yaml\", imgsz=640, epochs=1)\n",
    "trainer = RTDETRTrainer(overrides=args)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75035575",
   "metadata": {},
   "source": [
    "## Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ef4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(model='rtdetr-l.pt', data='coco8.yaml')\n",
    "validator = RTDETRValidator(args=args)\n",
    "validator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3302c0",
   "metadata": {},
   "source": [
    "## Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f694f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    " from ultralytics.utils import ASSETS\n",
    "        from ultralytics.models.rtdetr import RTDETRPredictor\n",
    "\n",
    "        args = dict(model='rtdetr-l.pt', source=ASSETS)\n",
    "        predictor = RTDETRPredictor(overrides=args)\n",
    "        predictor.predict_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a165fbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
