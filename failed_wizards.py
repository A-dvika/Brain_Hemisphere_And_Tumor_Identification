# -*- coding: utf-8 -*-
"""Failed_Wizards.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UBhyd9mrYz9FFvidxbLydFQyzcl00OEv
"""

!pip install pytorch-gradcam

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append("/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/pytorch-grad-cam-master/pytorch-grad-cam-master")

import time
import os
import cv2
import random
import torch.nn as nn
import numpy as np
import pandas as pd
import torch.backends.cudnn as cudnn
import torch.cuda.amp as amp
import matplotlib.pyplot as plt
import seaborn as sns
import torchvision.transforms as transforms
from torchvision.datasets import ImageFolder

from torch.utils.data import DataLoader
from torch.optim.lr_scheduler import CosineAnnealingLR

import sys
sys.path.append("/content/drive/MyDrive/Research_Project_Submission")

from src.zoo.swin_transformer import SwinTransformer
from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix
from torchsummary import summary

def rgb_to_lab_and_clahe(img, clip_limit, tile_grid_size, blur_sigma):
    """
    Convert an RGB image to LAB color space, apply CLAHE (Contrast Limited Adaptive Histogram Equalization).

    Args:
        img (PIL.Image): Input RGB image.
        clip_limit (float): Clip limit for CLAHE.
        tile_grid_size (tuple): Size of the tile grid for CLAHE.
        blur_sigma (float): Standard deviation for Gaussian blur.

    Returns:
        lab_image (numpy.ndarray): LAB color space image.
    """
    rgb_r, rgb_g, rgb_b = cv2.split(np.array(img))
    blurred_G = cv2.GaussianBlur(rgb_g, (5, 5), blur_sigma)
    blurred_B = cv2.GaussianBlur(rgb_b, (5, 5), blur_sigma)

    img = cv2.merge([rgb_r, blurred_G, blurred_B])
    lab_image = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2Lab)
    L, a, b = cv2.split(lab_image)
    clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)
    clahed_L = clahe.apply(L)
    modified_lab_image = cv2.merge([clahed_L, a, b])
    rgb_image = cv2.cvtColor(modified_lab_image, cv2.COLOR_LAB2RGB)
    return lab_image

train_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(degrees=180),
    transforms.GaussianBlur(kernel_size=5, sigma=(1.0, 3.0)),
    transforms.RandomAffine(degrees=0, translate=(0.25, 0.25), scale=(0.9, 1.1)),
    transforms.RandomPerspective(distortion_scale=0.5, p=0.5),
    lambda x: rgb_to_lab_and_clahe(x, clip_limit=5.0, tile_grid_size=(8, 8), blur_sigma=1.0),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),
])

val_transform = transforms.Compose([
    lambda x: rgb_to_lab_and_clahe(x, clip_limit=5.0, tile_grid_size=(8, 8), blur_sigma=1.0),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),
])

def mixup(data, targets, alpha=1.0):
    """
    Applies MixUp augmentation to the input data and targets.

    Args:
        data (torch.Tensor): Input data tensor.
        targets (torch.Tensor): Target labels tensor.
        alpha (float): MixUp hyperparameter controlling the mix ratio.

    Returns:
        mixed_data (torch.Tensor): Mixed data tensor.
        targets_a (torch.Tensor): Targets for the first data sample.
        targets_b (torch.Tensor): Targets for the second data sample.
        lam (float): Lambda value representing the mix ratio.
    """
    if alpha == 0:
        return data, targets, targets, 1.0
    lam = np.random.beta(alpha, alpha)
    batch_size = data.size(0)
    index = torch.randperm(batch_size)
    mixed_data = lam * data + (1 - lam) * data[index, :]
    targets_a, targets_b = targets, targets[index]
    return mixed_data, targets_a, targets_b, lam

"""# Data Loading"""

# Set the device for PyTorch to CUDA if available, otherwise use CPU.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Set a random seed for reproducibility and configure CUDA for benchmarking.
seed = 2
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
cudnn.benchmark = True

# Define the file paths for the training and validation datasets.
train_data_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/DATASET/train'
val_data_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/DATASET/val'

# Create PyTorch datasets with specified transformations.
train_dataset = ImageFolder(train_data_path, transform=train_transform)
val_dataset = ImageFolder(val_data_path, transform=val_transform)

# Set the batch size and create data loaders for training and validation.
batch_size = 8
train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True, num_workers=8)
val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=8)

!pip install timm

import sys
sys.path.append("/content/drive/MyDrive/Research_Project_Submission")

import torch
import torch.nn as nn
from timm.models.layers import DropPath, to_2tuple, trunc_normal_

class PatchEmbed(nn.Module):
    r""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops

class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,
                 fused_window_process=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer,
                                 fused_window_process=fused_window_process)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops


class PatchEmbed(nn.Module):
    r""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops

class SwinTransformer(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, fused_window_process=False, **kwargs):
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias, qk_scale=qk_scale,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint,
                               fused_window_process=fused_window_process)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def forward_features(self, x):
        x = self.patch_embed(x)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

    def flops(self):
        flops = 0
        flops += self.patch_embed.flops()
        for i, layer in enumerate(self.layers):
            flops += layer.flops()
        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)
        flops += self.num_features * self.num_classes
        return flops

sys.path.append("/content/drive/MyDrive/Research_Project_Submission/src/zoo")

"""# Model, Criterion & Optimizer"""

import torch
import torch.nn as nn
import torch.utils.checkpoint as checkpoint
from timm.models.layers import DropPath, to_2tuple, trunc_normal_


WindowProcess = None
WindowProcessReverse = None


class Mlp(nn.Module):
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fc1(x)
        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


def window_partition(x, window_size):
    """
    Args:
        x: (B, H, W, C)
        window_size (int): window size

    Returns:
        windows: (num_windows*B, window_size, window_size, C)
    """
    B, H, W, C = x.shape
    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows


def window_reverse(windows, window_size, H, W):
    """
    Args:
        windows: (num_windows*B, window_size, window_size, C)
        window_size (int): Window size
        H (int): Height of image
        W (int): Width of image

    Returns:
        x: (B, H, W, C)
    """
    B = int(windows.shape[0] / (H * W / window_size / window_size))
    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
    return x


class WindowAttention(nn.Module):
    r""" Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.

    Args:
        dim (int): Number of input channels.
        window_size (tuple[int]): The height and width of the window.
        num_heads (int): Number of attention heads.
        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set
        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0
        proj_drop (float, optional): Dropout ratio of output. Default: 0.0
    """

    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.dim = dim
        self.window_size = window_size  # Wh, Ww
        self.num_heads = num_heads
        head_dim = dim // num_heads
        self.scale = qk_scale or head_dim ** -0.5

        # define a parameter table of relative position bias
        self.relative_position_bias_table = nn.Parameter(
            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH

        # get pair-wise relative position index for each token inside the window
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww
        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2
        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww
        self.register_buffer("relative_position_index", relative_position_index)

        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(dim, dim)
        self.proj_drop = nn.Dropout(proj_drop)

        trunc_normal_(self.relative_position_bias_table, std=.02)
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, x, mask=None):
        """
        Args:
            x: input features with shape of (num_windows*B, N, C)
            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None
        """
        B_, N, C = x.shape
        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)

        q = q * self.scale
        attn = (q @ k.transpose(-2, -1))

        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(
            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww
        attn = attn + relative_position_bias.unsqueeze(0)

        if mask is not None:
            nW = mask.shape[0]
            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)
            attn = attn.view(-1, self.num_heads, N, N)
            attn = self.softmax(attn)
        else:
            attn = self.softmax(attn)

        attn = self.attn_drop(attn)

        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

    def extra_repr(self) -> str:
        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'

    def flops(self, N):
        # calculate flops for 1 window with token length of N
        flops = 0
        # qkv = self.qkv(x)
        flops += N * self.dim * 3 * self.dim
        # attn = (q @ k.transpose(-2, -1))
        flops += self.num_heads * N * (self.dim // self.num_heads) * N
        #  x = (attn @ v)
        flops += self.num_heads * N * N * (self.dim // self.num_heads)
        # x = self.proj(x)
        flops += N * self.dim * self.dim
        return flops


class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resulotion.
        num_heads (int): Number of attention heads.
        window_size (int): Window size.
        shift_size (int): Shift size for SW-MSA.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm,
                 fused_window_process=False):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.num_heads = num_heads
        self.window_size = window_size
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio
        if min(self.input_resolution) <= self.window_size:
            # if window size is larger than input resolution, we don't partition windows
            self.shift_size = 0
            self.window_size = min(self.input_resolution)
        assert 0 <= self.shift_size < self.window_size, "shift_size must in 0-window_size"

        self.norm1 = norm_layer(dim)
        self.attn = WindowAttention(
            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)

        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1

            mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)
            attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)
        self.fused_window_process = fused_window_process

    def forward(self, x):
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"

        shortcut = x
        x = self.norm1(x)
        x = x.view(B, H, W, C)

        # cyclic shift
        if self.shift_size > 0:
            if not self.fused_window_process:
                shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))
                # partition windows
                x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
            else:
                x_windows = WindowProcess.apply(x, B, H, W, C, -self.shift_size, self.window_size)
        else:
            shifted_x = x
            # partition windows
            x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C

        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C

        # W-MSA/SW-MSA
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C

        # merge windows
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)

        # reverse cyclic shift
        if self.shift_size > 0:
            if not self.fused_window_process:
                shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C
                x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
            else:
                x = WindowProcessReverse.apply(attn_windows, B, H, W, C, self.shift_size, self.window_size)
        else:
            shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C
            x = shifted_x
        x = x.view(B, H * W, C)
        x = shortcut + self.drop_path(x)

        # FFN
        x = x + self.drop_path(self.mlp(self.norm2(x)))

        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, " \
               f"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}"

    def flops(self):
        flops = 0
        H, W = self.input_resolution
        # norm1
        flops += self.dim * H * W
        # W-MSA/SW-MSA
        nW = H * W / self.window_size / self.window_size
        flops += nW * self.attn.flops(self.window_size * self.window_size)
        # mlp
        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio
        # norm2
        flops += self.dim * H * W
        return flops


class PatchMerging(nn.Module):
    r""" Patch Merging Layer.

    Args:
        input_resolution (tuple[int]): Resolution of input feature.
        dim (int): Number of input channels.
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
        super().__init__()
        self.input_resolution = input_resolution
        self.dim = dim
        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)
        self.norm = norm_layer(4 * dim)

    def forward(self, x):
        """
        x: B, H*W, C
        """
        H, W = self.input_resolution
        B, L, C = x.shape
        assert L == H * W, "input feature has wrong size"
        assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."

        x = x.view(B, H, W, C)

        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C
        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C
        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C
        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C
        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C
        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C

        x = self.norm(x)
        x = self.reduction(x)

        return x

    def extra_repr(self) -> str:
        return f"input_resolution={self.input_resolution}, dim={self.dim}"

    def flops(self):
        H, W = self.input_resolution
        flops = H * W * self.dim
        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim
        return flops


class BasicLayer(nn.Module):
    """ A basic Swin Transformer layer for one stage.

    Args:
        dim (int): Number of input channels.
        input_resolution (tuple[int]): Input resolution.
        depth (int): Number of blocks.
        num_heads (int): Number of attention heads.
        window_size (int): Local window size.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.
        drop (float, optional): Dropout rate. Default: 0.0
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0
        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm
        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.
        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False
    """

    def __init__(self, dim, input_resolution, depth, num_heads, window_size,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,
                 fused_window_process=False):

        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution
        self.depth = depth
        self.use_checkpoint = use_checkpoint

        # build blocks
        self.blocks = nn.ModuleList([
            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,
                                 num_heads=num_heads, window_size=window_size,
                                 shift_size=0 if (i % 2 == 0) else window_size // 2,
                                 mlp_ratio=mlp_ratio,
                                 qkv_bias=qkv_bias, qk_scale=qk_scale,
                                 drop=drop, attn_drop=attn_drop,
                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,
                                 norm_layer=norm_layer,
                                 fused_window_process=fused_window_process)
            for i in range(depth)])

        # patch merging layer
        if downsample is not None:
            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None

    def forward(self, x):
        for blk in self.blocks:
            if self.use_checkpoint:
                x = checkpoint.checkpoint(blk, x)
            else:
                x = blk(x)
        if self.downsample is not None:
            x = self.downsample(x)
        return x

    def extra_repr(self) -> str:
        return f"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}"

    def flops(self):
        flops = 0
        for blk in self.blocks:
            flops += blk.flops()
        if self.downsample is not None:
            flops += self.downsample.flops()
        return flops


class PatchEmbed(nn.Module):
    r""" Image to Patch Embedding

    Args:
        img_size (int): Image size.  Default: 224.
        patch_size (int): Patch token size. Default: 4.
        in_chans (int): Number of input image channels. Default: 3.
        embed_dim (int): Number of linear projection output channels. Default: 96.
        norm_layer (nn.Module, optional): Normalization layer. Default: None
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        img_size = to_2tuple(img_size)
        patch_size = to_2tuple(patch_size)
        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]
        self.img_size = img_size
        self.patch_size = patch_size
        self.patches_resolution = patches_resolution
        self.num_patches = patches_resolution[0] * patches_resolution[1]

        self.in_chans = in_chans
        self.embed_dim = embed_dim

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)
        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

    def forward(self, x):
        B, C, H, W = x.shape
        # FIXME look at relaxing size constraints
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x).flatten(2).transpose(1, 2)  # B Ph*Pw C
        if self.norm is not None:
            x = self.norm(x)
        return x

    def flops(self):
        Ho, Wo = self.patches_resolution
        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])
        if self.norm is not None:
            flops += Ho * Wo * self.embed_dim
        return flops


class SwinTransformer(nn.Module):
    r""" Swin Transformer
        A PyTorch impl of : `Swin Transformer: Hierarchical Vision Transformer using Shifted Windows`  -
          https://arxiv.org/pdf/2103.14030

    Args:
        img_size (int | tuple(int)): Input image size. Default 224
        patch_size (int | tuple(int)): Patch size. Default: 4
        in_chans (int): Number of input image channels. Default: 3
        num_classes (int): Number of classes for classification head. Default: 1000
        embed_dim (int): Patch embedding dimension. Default: 96
        depths (tuple(int)): Depth of each Swin Transformer layer.
        num_heads (tuple(int)): Number of attention heads in different layers.
        window_size (int): Window size. Default: 7
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4
        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None
        drop_rate (float): Dropout rate. Default: 0
        attn_drop_rate (float): Attention dropout rate. Default: 0
        drop_path_rate (float): Stochastic depth rate. Default: 0.1
        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.
        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False
        patch_norm (bool): If True, add normalization after patch embedding. Default: True
        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False
        fused_window_process (bool, optional): If True, use one kernel to fused window shift & window partition for acceleration, similar for the reversed part. Default: False
    """

    def __init__(self, img_size=224, patch_size=4, in_chans=3, num_classes=1000,
                 embed_dim=96, depths=[2, 2, 6, 2], num_heads=[3, 6, 12, 24],
                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,
                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,
                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,
                 use_checkpoint=False, fused_window_process=False, **kwargs):
        super().__init__()

        self.num_classes = num_classes
        self.num_layers = len(depths)
        self.embed_dim = embed_dim
        self.ape = ape
        self.patch_norm = patch_norm
        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))
        self.mlp_ratio = mlp_ratio

        # split image into non-overlapping patches
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim,
            norm_layer=norm_layer if self.patch_norm else None)
        num_patches = self.patch_embed.num_patches
        patches_resolution = self.patch_embed.patches_resolution
        self.patches_resolution = patches_resolution

        # absolute position embedding
        if self.ape:
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))
            trunc_normal_(self.absolute_pos_embed, std=.02)

        self.pos_drop = nn.Dropout(p=drop_rate)

        # stochastic depth
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule

        # build layers
        self.layers = nn.ModuleList()
        for i_layer in range(self.num_layers):
            layer = BasicLayer(dim=int(embed_dim * 2 ** i_layer),
                               input_resolution=(patches_resolution[0] // (2 ** i_layer),
                                                 patches_resolution[1] // (2 ** i_layer)),
                               depth=depths[i_layer],
                               num_heads=num_heads[i_layer],
                               window_size=window_size,
                               mlp_ratio=self.mlp_ratio,
                               qkv_bias=qkv_bias, qk_scale=qk_scale,
                               drop=drop_rate, attn_drop=attn_drop_rate,
                               drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],
                               norm_layer=norm_layer,
                               downsample=PatchMerging if (i_layer < self.num_layers - 1) else None,
                               use_checkpoint=use_checkpoint,
                               fused_window_process=fused_window_process)
            self.layers.append(layer)

        self.norm = norm_layer(self.num_features)
        self.avgpool = nn.AdaptiveAvgPool1d(1)
        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()

        self.apply(self._init_weights)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            trunc_normal_(m.weight, std=.02)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            nn.init.constant_(m.bias, 0)
            nn.init.constant_(m.weight, 1.0)

    @torch.jit.ignore
    def no_weight_decay(self):
        return {'absolute_pos_embed'}

    @torch.jit.ignore
    def no_weight_decay_keywords(self):
        return {'relative_position_bias_table'}

    def forward_features(self, x):
        x = self.patch_embed(x)
        if self.ape:
            x = x + self.absolute_pos_embed
        x = self.pos_drop(x)

        for layer in self.layers:
            x = layer(x)

        x = self.norm(x)  # B L C
        x = self.avgpool(x.transpose(1, 2))  # B C 1
        x = torch.flatten(x, 1)
        return x

    def forward(self, x):
        x = self.forward_features(x)
        x = self.head(x)
        return x

    def flops(self):
        flops = 0
        flops += self.patch_embed.flops()
        for i, layer in enumerate(self.layers):
            flops += layer.flops()
        flops += self.num_features * self.patches_resolution[0] * self.patches_resolution[1] // (2 ** self.num_layers)
        flops += self.num_features * self.num_classes
        return flops

swin_config = {
    'img_size': 224,
    'patch_size': 4,
    'in_chans': 3,
    'num_classes': 2,
    'embed_dim': 96,
    'depths': [2, 2, 6, 2],
    'num_heads': [3, 6, 12, 24],
    'window_size': 7,
    'mlp_ratio': 4,
    'stochastic_depth_prob': 0.2,
}

# Instantiate Swin Transformer model.
model = SwinTransformer(img_size=swin_config['img_size'],
                        patch_size=swin_config['patch_size'],
                        in_chans=swin_config['in_chans'],
                        num_classes=swin_config['num_classes'],
                        embed_dim=swin_config['embed_dim'],
                        depths=swin_config['depths'],
                        num_heads=swin_config['num_heads'],
                        window_size=swin_config['window_size'],
                        mlp_ratio=swin_config['mlp_ratio'],
                        qkv_bias=True,
                        qk_scale=None,
                        drop_rate=0.0,
                        drop_path_rate=0.1,
                        ape=False,
                        patch_norm=True,
                        use_checkpoint=False,
                        fused_window_process=False).to(device)

# Generate a summary of the model's architecture
summary(model, (3, 224, 224))

# Set up the Cross Entropy loss function
criterion = nn.CrossEntropyLoss()

# Set the Optimizer as AdamW
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=7.8125e-06,
    betas=(0.9, 0.999),
    eps=1.0e-08,
    weight_decay=0.05,
)

# Set the Scheduler
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=30,
    eta_min=7.8125e-08
)

# Set gradient scaler
scaler = amp.GradScaler()

from PIL import Image
import os

# Set the input directory
input_dir = "/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Detection/images/val"

# Set the target size (224x224)
target_size = (224, 224)

# Loop through the files in the input directory
for filename in os.listdir(input_dir):
    if filename.endswith(".jpg"):  # Adjust the file extension as needed
        # Open the image using PIL
        with Image.open(os.path.join(input_dir, filename)) as img:
            # Resize the image in place (overwrite the original)
            img.thumbnail(target_size, Image.ANTIALIAS)
            img.save(os.path.join(input_dir, filename), "JPEG")  # Save as JPEG

print("Images resized and overwritten in", input_dir)

# Initialize training parameters and lists to track metrics.
num_epochs = 2
best_accuracy = 0.0
best_recall = 0.0
best_f1_score = 0.0
best_epoch_accuracy = 0
best_model_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save'
train_losses = []
val_losses = []
val_accuracies = []

# Main training loop over epochs.
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    start_time = time.time()

    # Training loop over batches.
    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        # Apply MixUp augmentation.
        if np.random.rand() < 0.5:
            images, targets_a, targets_b, lam = mixup(images, labels, alpha=1.0)
        else:
            images, targets_a, targets_b, lam = mixup(images, labels, alpha=1.0)
        optimizer.zero_grad()

        with amp.autocast():
            outputs = model(images)
            #loss = criterion(outputs, labels)
            loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)

        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        #scheduler.step()

        running_loss += loss.item()

    train_losses.append(running_loss/len(train_loader))
    end_time = time.time()
    epoch_time = end_time - start_time
    print("############################################################")
    print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {running_loss/len(train_loader)}, Time: {epoch_time:.2f} seconds")

    if (epoch + 1) % 2 == 0:
        model.eval()
        correct = 0
        total = 0
        val_predictions = []
        val_labels = []
        running_val_loss = 0.0

        # Validation loop to evaluate model performance.
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                val_predictions.extend(predicted.cpu().numpy())
                val_labels.extend(labels.cpu().numpy())

                val_loss = criterion(outputs, labels)
                running_val_loss += val_loss.item()

        accuracy = 100 * correct / total
        val_losses.append(running_val_loss/len(val_loader))

        # Print and save model performance metrics.
        print(f"Val Loss: {running_val_loss/len(val_loader)}")
        val_accuracies.append(accuracy)
        recall = recall_score(val_labels, val_predictions)
        f1 = f1_score(val_labels, val_predictions)

        print(f"Current Validation Accuracy: {accuracy}%")
        print(f"Current Validation Recall: {recall}")
        print(f"Current Validation F1 Score: {f1}")

        # Save the model.
        model_path = os.path.join(best_model_path, f"model_classify_{epoch + 1}.pth")
        torch.save(model.state_dict(), model_path)
        print("Model Saved!")

# Plot training and validation metrics.
plt.figure(figsize=(12, 3))
plt.subplot(1, 3, 1)
plt.plot(range(1, num_epochs + 1, 1), train_losses, label='Train Loss', color='blue')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(range(2, num_epochs + 1, 2), val_losses, label='Validation Loss', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(range(2, num_epochs + 1, 2), val_accuracies, label='Validation Accuracy', color='green')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.legend()
plt.show()

if (epoch + 1) % 2 == 0:
        model.eval()
        correct = 0
        total = 0
        val_predictions = []
        val_labels = []
        running_val_loss = 0.0

        # Validation loop to evaluate model performance.
        with torch.no_grad():
            for images, labels in val_loader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                val_predictions.extend(predicted.cpu().numpy())
                val_labels.extend(labels.cpu().numpy())

                val_loss = criterion(outputs, labels)
                running_val_loss += val_loss.item()

        accuracy = 100 * correct / total
        val_losses.append(running_val_loss/len(val_loader))

        # Print and save model performance metrics.
        print(f"Val Loss: {running_val_loss/len(val_loader)}")
        val_accuracies.append(accuracy)
        recall = recall_score(val_labels, val_predictions)
        f1 = f1_score(val_labels, val_predictions)

        print(f"Current Validation Accuracy: {accuracy}%")
        print(f"Current Validation Recall: {recall}")
        print(f"Current Validation F1 Score: {f1}")

        # Save the model.
        model_path = os.path.join(best_model_path, f"model_classify_{epoch + 1}.pth")
        torch.save(model.state_dict(), model_path)
        print("Model Saved!")

# Plot training and validation metrics.
plt.figure(figsize=(12, 3))
plt.subplot(1, 3, 1)
plt.plot(range(1, num_epochs + 1, 1), train_losses, label='Train Loss', color='blue')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(range(2, num_epochs + 1, 2), val_losses, label='Validation Loss', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(range(2, num_epochs + 1, 2), val_accuracies, label='Validation Accuracy', color='green')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.legend()
plt.show()

best_model_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save'
print(f"Current Validation Accuracy: {accuracy}%")
print(f"Current Validation Recall: {recall}")
print(f"Current Validation F1 Score: {f1}")

# Save the model.
model_path = os.path.join(best_model_path, f"model_classify_{epoch + 1}.pth")
torch.save(model.state_dict(), model_path)
print("Model Saved!")

# Plot training and validation metrics.
plt.figure(figsize=(12, 3))
plt.subplot(1, 3, 1)
plt.plot(range(1, num_epochs + 1, 1), train_losses, label='Train Loss', color='blue')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.legend()

plt.subplot(1, 3, 2)
plt.plot(range(2, num_epochs + 1, 2), val_losses, label='Validation Loss', color='orange')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.legend()

plt.subplot(1, 3, 3)
plt.plot(range(2, num_epochs + 1, 2), val_accuracies, label='Validation Accuracy', color='green')
plt.xlabel('Epoch')
plt.ylabel('Accuracy (%)')
plt.gca().xaxis.set_major_locator(plt.MaxNLocator(integer=True))
plt.legend()
plt.show()

#Defining data preprocessing for test data.
test_transform = transforms.Compose([
    lambda x: rgb_to_lab_and_clahe(x, clip_limit=5.0, tile_grid_size=(8, 8), blur_sigma=1.0),
    transforms.ToTensor(),
    transforms.Normalize(mean=(0.0, 0.0, 0.0), std=(1.0, 1.0, 1.0)),
])

#Defining path for saving test prediction, model checkpoint, CAMs, and Excel file
test_data_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/DATASET/val'
model_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/model_classify_2.pth'
cams_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/save/cams/'
excel_file_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/predictions_classify.xlsx'
batch_size = 1

# Create a test dataset and data loader for inference.
test_dataset = ImageFolder(test_data_path, transform=test_transform)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=8)

# Define a function to reshape a tensor for Ablation-CAM function
def reshape_transform(tensor, height=7, width=7):
    """
    Reshape a tensor to the specified height and width dimensions.

    Args:
        tensor (torch.Tensor): Input tensor.
        height (int): Target height dimension.
        width (int): Target width dimension.

    Returns:
        result (torch.Tensor): Reshaped tensor.
    """
    result = tensor.reshape(tensor.size(0), height, width, tensor.size(2))
    result = result.transpose(2, 3).transpose(1, 2)
    return result


# Load a pre-trained model and set it to evaluation mode.
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model.load_state_dict(torch.load(model_path))
model.eval()

# Define class names, fonts, and variables for predictions and image file names.
class_names = ['Axial', 'Coronal']
i = 0
font = cv2.FONT_HERSHEY_SIMPLEX
font_scale = 0.5
font_color = (255, 255, 255)
font_thickness = 1
predicted_labels = []
image_file_names = []

# Loop through test data and generate predictions.
for images, _ in test_loader:
    images = images.to(device)
    outputs = model(images)
    _, predicted = torch.max(outputs.data, 1)
    predicted_label = class_names[predicted.item()]

## Uncomment this code for generating Ablation-CAM plots.
    # target_layers = [model.layers[-1].blocks[-1].norm2]
    # cam = AblationCAM(model=model, target_layers=target_layers, reshape_transform=reshape_transform, ablation_layer=AblationLayerVit())
    # grayscale_cam = cam(input_tensor=images, aug_smooth=True, eigen_smooth=True, targets=None)
    # rgb_img = cv2.imread(test_loader.dataset.samples[i][0], 1)[:, :, ::-1]
    # rgb_img_n = np.float32(rgb_img) / 255
    # cam_image = show_cam_on_image(rgb_img_n, grayscale_cam[0, :])

    # predicted_labels.append(predicted_label)
    # image_file_names.append(os.path.basename(test_loader.dataset.samples[i][0]))

    # combined_image = np.copy(cam_image)
    # text = f'Predicted: {predicted_label}'
    # cv2.putText(combined_image, text, (15, 15), font, font_scale, font_color, font_thickness)
    # cv2.imwrite(os.path.join(cams_path, os.path.basename(test_loader.dataset.samples[i][0])), combined_image)

    ## Append predicted labels and image file names.
    file_name = os.path.splitext(os.path.basename(test_loader.dataset.samples[i][0]))[0]
    image_file_names.append(file_name)
    predicted_labels.append(predicted.item())
    i += 1

# Create a DataFrame and save predictions to an Excel file.
data = {
    'Image name': image_file_names,
    'Predicted Class label': predicted_labels,
}
df = pd.DataFrame(data)
df['Predicted Class label'] = df['Predicted Class label'].map({0: 'Axial', 1: 'Coronal'})

df.to_excel(excel_file_path, index=False)
print(f"Predictions saved to {excel_file_path}")

"""# Detection"""

import os
import json
import cv2
from tqdm import tqdm

def yolo_to_coco(yolo_labels_path, images_folder, coco_json_path, categories):
    coco_data = {
        "images": [],
        "annotations": [],
        "categories": categories,
    }

    image_id = 1
    annotation_id = 1

    # Iterate over the YOLO label files
    for root, _, files in os.walk(yolo_labels_path):
        for file in tqdm(files, desc="Converting YOLO labels to COCO format"):
            if file.endswith('.txt'):
                image_filename = os.path.splitext(file)[0] + ".jpg"
                image_path = os.path.join(images_folder, image_filename)

                img = cv2.imread(image_path)
                height, width, _ = img.shape

                image_info = {
                    "id": image_id,
                    "file_name": image_filename,
                    "width": width,
                    "height": height
                }
                coco_data["images"].append(image_info)

                with open(os.path.join(root, file), 'r') as label_file:
                    lines = label_file.readlines()

                for line in lines:
                    values = line.strip().split()
                    category_id = int(values[0])
                    x_center, y_center, box_width, box_height = map(float, values[1:])

                    x_min = max(0, int((x_center - box_width / 2) * width))
                    y_min = max(0, int((y_center - box_height / 2) * height))
                    x_max = min(width, int((x_center + box_width / 2) * width))
                    y_max = min(height, int((y_center + box_height / 2) * height))

                    annotation = {
                        "id": annotation_id,
                        "image_id": image_id,
                        "category_id": category_id,
                        "bbox": [x_min, y_min, x_max - x_min, y_max - y_min],
                        "area": (x_max - x_min) * (y_max - y_min),
                        "iscrowd": 0
                    }
                    coco_data["annotations"].append(annotation)

                    annotation_id += 1

                image_id += 1

    # Save the COCO data to a JSON file
    with open(coco_json_path, 'w') as json_file:
        json.dump(coco_data, json_file)

# Define your YOLO label folder, images folder, COCO JSON path, and category list
yolo_labels_path = "/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Detection/labels/val"  # Path to YOLO label files for training set
images_folder = "/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Detection/images/val"     # Path to training images
coco_json_path = "/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Detection/coco_format_val.json"     # Path to save the COCO JSON file for training set
categories = [
    {"id": 1, "name": "non tumor"},
    {"id": 2, "name": "tumor"},
    # Add more categories as needed
]

# Convert YOLO to COCO format
yolo_to_coco(yolo_labels_path, images_folder, coco_json_path, categories)

print("Conversion to COCO format completed.")



!pip install onnxruntime

!pip install onnx

sys.path.append("/content/drive/MyDrive/Research_Project_Submission/")

#import src.misc.dist as dist
import torch
import onnx
import os
import torch.nn as nn
from src.core import YAMLConfig
from src.solver import TASKS
import onnxruntime as ort
from PIL import Image, ImageDraw, ImageFont
from torchvision.transforms import ToTensor

!pip install pyyaml

config_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Detection/rtdetr/rtdetr_r101vd_6x_coco.yml'
file_name = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/model.onnx'
save_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/figs'

#dist.init_distributed(backend='nccl')

cfg = YAMLConfig(config_path, use_amp=True)

solver = TASKS[cfg.yaml_cfg['task']](cfg)

solver.fit()

#Uncomment this for validation
#solver.val()

resume_path="/content/drive/MyDrive/logs/checkpoint.pth"

# Load model configuration from a YAML file and resume training from a checkpoint
cfg = YAMLConfig(config_path, resume=resume_path)
checkpoint = torch.load(resume_path, map_location='cpu')

# Extract the model state from the checkpoint
if 'ema' in checkpoint:
    state = checkpoint['ema']['module']
else:
    state = checkpoint['model']


cfg.model.load_state_dict(state)

# Define a custom model class and deploy the model and postprocessor components
class Model(nn.Module):
    def __init__(self, ) -> None:
        super().__init__()
        self.model = cfg.model.deploy()
        self.postprocessor = cfg.postprocessor.deploy()
        print(self.postprocessor.deploy_mode)

    def forward(self, images, orig_target_sizes):
        outputs = self.model(images)
        return self.postprocessor(outputs, orig_target_sizes)


model = Model()

# Define dynamic axes for input and export model to ONNX format
dynamic_axes = {
    'images': {0: 'N', },
    'orig_target_sizes': {0: 'N'}
}

data = torch.rand(1, 3, 640, 640)
size = torch.tensor([[640, 640]])

torch.onnx.export(
    model,
    (data, size),
    file_name,
    input_names=['images', 'orig_target_sizes'],
    output_names=['labels', 'boxes', 'scores'],
    dynamic_axes=dynamic_axes,
    opset_version=16,
    verbose=False
)
# Load the exported ONNX model and perform a validation check
onnx_model = onnx.load(file_name)
onnx.checker.check_model(onnx_model)
print('Check export onnx model done...')

# Define the file paths for the training and validation datasets.
train_data_path =
val_data_path =

import numpy as np
import matplotlib.pylab as plt


import tensorflow as tf

from tensorflow import keras

import tensorflow_hub as hub

IMAGE_SHAPE = (224, 224)

image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)

training_data=r"/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/DATASET/train"
training_image_data  = image_generator.flow_from_directory(training_data,target_size=IMAGE_SHAPE)

training_image_data[1][1].shape

validation_data=r'/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/DATASET/val'
validation_image_data  = image_generator.flow_from_directory(validation_data,target_size=IMAGE_SHAPE)

class_names = sorted(training_image_data.class_indices.items(), key=lambda pair:pair[1])
class_names = np.array([key.title() for key, value in class_names])
class_names

MobileNetV2=tf.keras.applications.mobilenet_v2.MobileNetV2
model_arch=MobileNetV2()
model_arch.summary()

feature_extractor_model = "https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4"

feature_extractor = hub.KerasLayer(feature_extractor_model, input_shape=(224, 224, 3), trainable=False)

model = tf.keras.Sequential([feature_extractor])
# model.add(tf.keras.layers.GlobalAveragePooling2D())
model.add(tf.keras.layers.Dense(6, activation='softmax'))

for image_batch, label_batch in training_image_data:
  print("Image batch shape: ", image_batch.shape)
  print("Label batch shape: ", label_batch.shape)
  break

model = tf.keras.Sequential([feature_extractor ])
model.add(tf.keras.layers.Dense(2))
model.summary()

class CollectBatchStats(tf.keras.callbacks.Callback):
  def __init__(self):
    self.batch_losses = []
    self.batch_acc = []

  def on_train_batch_end(self, batch, logs=None):
    self.batch_losses.append(logs['loss'])
    self.batch_acc.append(logs['acc'])
    self.model.reset_metrics()

batch_stats_callback = CollectBatchStats()

batch_size =  32

num_epochs = 10

total_samples = len(training_image_data)
steps_per_epoch = 10

history = model.fit(training_image_data, epochs=num_epochs,
                    validation_data=validation_image_data,
                    steps_per_epoch=steps_per_epoch,
                    batch_size=batch_size,
                    callbacks=[batch_stats_callback])

import torch
import torch.nn as nn
from torchvision import datasets, transforms
from efficientnet_pytorch import EfficientNet

# Define file paths for the training and validation datasets
train_data_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/DATASET/train'
val_data_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/DATASET/val'

# Define transforms for preprocessing the data
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))  # ImageNet normalization
])

# Create datasets
train_dataset = datasets.ImageFolder(train_data_path, transform=transform)
val_dataset = datasets.ImageFolder(val_data_path, transform=transform)

# Define data loaders
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32)

# Use EfficientNet model
model = EfficientNet.from_name('efficientnet-b0', num_classes=2)  # Assuming binary classification

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Training loop
epochs = 5
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    for inputs, labels in train_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
    print(f"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}")

# Save the trained model
model_save_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/model_classify_efficientnet.pth'
torch.save(model.state_dict(), model_save_path)
print("Model saved successfully!")

sys.path.append("/content/drive/MyDrive/Research_Project_Submission/src")

import torch
import os
import cv2
import onnxruntime as ort
from PIL import Image, ImageDraw, ImageFont
from torchvision.transforms import ToTensor
from src.zoo.swin_transformer import SwinTransformer

import torch

"""Inference"""

image_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/DATASET/train/Axial/00053_207_jpg.rf.f1000566376fde541146af70dac12fe1.jpg'
model_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/model_classify_2.pth'
file_name = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/model.onnx'
save_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/figs'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define the Swin Transformer model configuration.
swin_config = {
    'img_size': 224,
    'patch_size': 4,
    'in_chans': 3,
    'num_classes': 2,
    'embed_dim': 96,
    'depths': [2, 2, 6, 2],
    'num_heads': [3, 6, 12, 24],
    'window_size': 7,
    'mlp_ratio': 4,
    'stochastic_depth_prob': 0.2,
}

# Instantiate Swin Transformer model.
model = SwinTransformer(img_size=swin_config['img_size'],
                        patch_size=swin_config['patch_size'],
                        in_chans=swin_config['in_chans'],
                        num_classes=swin_config['num_classes'],
                        embed_dim=swin_config['embed_dim'],
                        depths=swin_config['depths'],
                        num_heads=swin_config['num_heads'],
                        window_size=swin_config['window_size'],
                        mlp_ratio=swin_config['mlp_ratio'],
                        qkv_bias=True,
                        qk_scale=None,
                        drop_rate=0.0,
                        drop_path_rate=0.1,
                        ape=False,
                        patch_norm=True,
                        use_checkpoint=False,
                        fused_window_process=False).to(device)

model.load_state_dict(torch.load(model_path))
model.eval()

# Define class names
class_names = ['Axial', 'Coronal']
font = cv2.FONT_HERSHEY_SIMPLEX
font_scale = 0.5
font_color = (255, 255, 255)
font_thickness = 1

# Function to predict whether an image is bleeding or not
def predict_image(image_path):
    image = cv2.imread(image_path)
    image_tensor = torch.tensor(image, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0) / 255.0
    image_tensor = image_tensor.to(device)
    with torch.no_grad():
        output = model(image_tensor)
        _, predicted = torch.max(output.data, 1)
    predicted_label = class_names[predicted.item()]

    return predicted_label

# Predict the label for the input image.
predicted_label = predict_image(image_path)


# Check if the image is classified as bleeding
if predicted_label == 'Axial':
    # Initialize ONNX inference session.
    sess = ort.InferenceSession(file_name)

    # Threshold for object detection confidence
    thrh = 0.4
    # Use a built-in font for labeling
    from PIL import ImageFont
    fnt = ImageFont.load_default()

    # Load and preprocess the image
    im = Image.open(image_path).convert('RGB')
    im = im.resize((640, 640))
    im_data = ToTensor()(im)[None]
    size = torch.tensor([[640, 640]])

    # Perform inference
    output = sess.run(
        output_names=['labels', 'boxes', 'scores'],
        input_feed={'images': im_data.data.numpy(), "orig_target_sizes": size.data.numpy()}
    )

    labels, boxes, scores = output

    # Create an annotated image
    draw = ImageDraw.Draw(im)

    for i in range(im_data.shape[0]):
        scr = scores[i]
        lab = labels[i][scr > thrh]
        box = boxes[i][scr > thrh]

        for j, b in enumerate(box):
            label = lab[j]
            confidence = scr[j]

            if label == 1:
                draw.rectangle(list(b), outline='blue', width=7)
                draw.text((b[0], b[1]), text=f"Axial({confidence:.2f})", font=fnt, fill='yellow', width=100)
            else:
                draw.rectangle(list(b), outline='blue', width=7)
                draw.text((b[0], b[1]), text=f"{label} ({confidence:.2f})", font=fnt, fill='yellow', width=100)



    #annotated image with the same name
    im = im.resize((224, 224))
    save_filename = os.path.join(save_path, os.path.basename(image_path))
    im.save(save_filename)

    print("Image is tumor and bounding box annotation is complete.")
else:
    print("The image is classified as non-tumor")

import torch
import onnxruntime as ort
import cv2
from PIL import Image, ImageDraw
import os
from torchvision.transforms.functional import to_tensor as ToTensor

# Define the Swin Transformer model and other configurations here...

# Function to predict whether an image is bleeding or not
def predict_image(image_path):
    # ... (Code to load and preprocess image remains the same as before)

    return predicted_label

# Paths and configurations
image_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Classification/DATASET/train/Axial/00053_207_jpg.rf.f1000566376fde541146af70dac12fe1.jpg'
model_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/model_classify_2.pth'
file_name = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/model.onnx'
save_path = '/content/drive/MyDrive/Research_Project_Submission/Brain_Tumor_Detection/Save/figs'

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ... (Define the Swin Transformer model configuration and instantiation here)

model.load_state_dict(torch.load(model_path))
model.eval()

# Define class names
class_names = ['Axial', 'Coronal']
font = cv2.FONT_HERSHEY_SIMPLEX
font_scale = 0.5
font_color = (255, 255, 255)
font_thickness = 1

# Function to perform inference and annotate the image with bounding boxes
def annotate_image(image_path, predicted_label):
    if predicted_label == 'Axial':
        # Initialize ONNX inference session.
        sess = ort.InferenceSession(file_name)

        # ... (Rest of the code for object detection remains the same)

        # Perform inference
        output = sess.run(
            output_names=['labels', 'boxes', 'scores'],
            input_feed={'images': im_data.data.numpy(), "orig_target_sizes": size.data.numpy()}
        )

        labels, boxes, scores = output

        # Create an annotated image
        draw = ImageDraw.Draw(im)

        for i in range(im_data.shape[0]):
            scr = scores[i]
            lab = labels[i][scr > thrh]
            box = boxes[i][scr > thrh]

            for j, b in enumerate(box):
                label = lab[j]
                confidence = scr[j]

                if label == 1:  # Assuming label 1 corresponds to the object of interest
                    draw.rectangle(list(b), outline='blue', width=7)
                    draw.text((b[0], b[1]), text=f"Axial({confidence:.2f})", font=fnt, fill='yellow', width=100)
                else:
                    draw.rectangle(list(b), outline='blue', width=7)
                    draw.text((b[0], b[1]), text=f"{label} ({confidence:.2f})", font=fnt, fill='yellow', width=100)

        # Save annotated image with bounding boxes
        im = im.resize((224, 224))
        save_filename = os.path.join(save_path, os.path.basename(image_path))
        im.save(save_filename)

        print("Image is bleeding, and bounding box annotation is complete.")
    else:
        print("The image is classified as non-bleeding.")

# Predict the label for the input image.
predicted_label = predict_image(image_path)

# Annotate image if 'Axial' classification
annotate_image(image_path, predicted_label)